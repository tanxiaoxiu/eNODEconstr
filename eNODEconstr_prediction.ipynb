{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys, copy, math, time, pdb\n",
    "import os.path\n",
    "import random\n",
    "import pdb\n",
    "import csv\n",
    "import argparse\n",
    "import itertools\n",
    "from itertools import permutations, product\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torchdiffeq import odeint\n",
    "import itertools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import rpy2.robjects as robjects\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "N = 10\n",
    "M = 10\n",
    "device = torch.device(\"cpu\")\n",
    "t5 = torch.tensor([0, 2, 6, 14, 24], dtype=torch.float32)\n",
    "t10 = torch.tensor([0, 1, 2, 6, 9, 12, 15, 18, 21, 24], dtype=torch.float32)\n",
    "t15 =  torch.tensor([0, 1, 2, 3, 4, 5, 7, 9, 11, 13, 15, 17, 19, 21, 24], dtype=torch.float32)\n",
    "t20 = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 18, 20, 22, 24], dtype=torch.float32)\n",
    "t25 = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], dtype=torch.float32)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "    def __getitem__(self, idx):\n",
    "        input_data = self.tensor[idx, 0, :]\n",
    "        output_data = self.tensor[idx, :, :]\n",
    "        return input_data, output_data\n",
    "\n",
    "def loss_fn(pred_y, y):\n",
    "    return torch.mean(torch.sum(torch.square(y - pred_y)))\n",
    "        \n",
    "class ODEFunc(torch.nn.Module):\n",
    "    def __init__(self, N, M, mu_base):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        self.N = N \n",
    "        self.M = M\n",
    "        \n",
    "        self.mu = torch.nn.Parameter(torch.rand(N, M, device=device))\n",
    "\n",
    "        self.mu_base = mu_base\n",
    "        self.mu_mask = (mu_base != 0).float()\n",
    "\n",
    "        self.lambda_ = torch.nn.Parameter(torch.rand(1, device=device))\n",
    "        self.m = torch.nn.Parameter(torch.rand(N, device=device))\n",
    "        self.rho = torch.nn.Parameter(torch.rand(M, device=device))\n",
    "        self.omega = torch.nn.Parameter(torch.rand(M, device=device))\n",
    "\n",
    "        nonzero_byproduct=0.5\n",
    "        self.l = torch.nn.Parameter(self.generate_byproduct_matrix(M, nonzero_byproduct, device))\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_byproduct_matrix(M, nonzero_byproduct, device):\n",
    "        num_elements = M * M\n",
    "        num_nonzeros = int(num_elements * nonzero_byproduct)\n",
    "        values = np.concatenate([np.random.uniform(0, 1, num_nonzeros), np.zeros(num_elements - num_nonzeros)])\n",
    "        np.random.shuffle(values)\n",
    "        D = torch.tensor(values.reshape(M, M), dtype=torch.float32, device=device)\n",
    "        col_sums = torch.sum(D, axis=0)\n",
    "        col_sums[col_sums == 0] = 1\n",
    "        D_norm = D / col_sums\n",
    "        return D_norm\n",
    "\n",
    "    def forward(self, t, ys):\n",
    "        dydts_list = []\n",
    "        for y in ys: \n",
    "            N, M = self.N, self.M\n",
    "            C, R = y[:N], y[N:]\n",
    "            C = C.unsqueeze(0)\n",
    "            R = R.unsqueeze(0)\n",
    "\n",
    "            dCdt = C * (R @ self.mu.t() * (1 - self.lambda_)) - C * self.m\n",
    "            dRdt = self.rho - R * self.omega - (C @ self.mu) * R + self.lambda_ * ((C @ self.mu) * R) @ self.l.t()\n",
    "            dydt = torch.cat([dCdt.squeeze(0), dRdt.squeeze(0)])\n",
    "            dydt = dydt * (y > 0).float()\n",
    "            dydts_list.append(dydt)\n",
    "\n",
    "        dydts = torch.stack(dydts_list)\n",
    "        return dydts\n",
    "        \n",
    "    def constrained(self):\n",
    "        with torch.no_grad():\n",
    "            self.lambda_.data = torch.relu(self.lambda_.data)\n",
    "            self.m.data = torch.relu(self.m.data)\n",
    "            self.rho.data = torch.relu(self.rho.data)\n",
    "            self.omega.data = torch.relu(self.omega.data)\n",
    "            self.l.data = torch.relu(self.l.data)\n",
    "\n",
    "def calculate_r_rmse(true_new_state, predict_new_state, S, N, M):\n",
    "    results = []\n",
    "\n",
    "    subjects = [f\"Subject{i+1}\" for i in range(S)]\n",
    "    regulates = [f\"Microbe{i+1}\" for i in range(N)] \n",
    "\n",
    "    for k in range(S):\n",
    "        for n in range(N): \n",
    "            true_state = true_new_state[k, n]\n",
    "            predict_state = predict_new_state[k, n]\n",
    "            C0 = true_state[:, :N]\n",
    "            R0 = true_state[:, N:N+M]\n",
    "            C1 = predict_state[:, :N]\n",
    "            R1 = predict_state[:, N:N+M]\n",
    "            C_r_rmse = np.sqrt(np.mean((C0 - C1) ** 2))/np.sqrt(np.mean((C0) ** 2))\n",
    "            R_r_rmse = np.sqrt(np.mean((R0 - R1) ** 2))/np.sqrt(np.mean((R0) ** 2))\n",
    "\n",
    "            results.append({\n",
    "                'Subject': subjects[k],\n",
    "                'Regulate': regulates[n],\n",
    "                'C_r_rmse': C_r_rmse,\n",
    "                'R_r_rmse': R_r_rmse\n",
    "            })\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "def calculate_L2_Distance(true_initial_state, predict_new_state, S, N, M):\n",
    "    results = []\n",
    "\n",
    "    for k in range(S):\n",
    "        true_initial_state_del = true_initial_state[k]\n",
    "\n",
    "        C0 = true_initial_state_del[:, :N]\n",
    "        R0 = true_initial_state_del[:, N:(N+M)]\n",
    "\n",
    "        predict_new_state_subject = predict_new_state[k]\n",
    "\n",
    "        C_distance = np.zeros(N)\n",
    "        R_distance = np.zeros(N)\n",
    "\n",
    "        for n in range(N):\n",
    "            predict_new_state_subject_del = predict_new_state_subject[n]\n",
    "            C1 = predict_new_state_subject_del[:, :N]\n",
    "            R1 = predict_new_state_subject_del[:, N:(N+M)]\n",
    "\n",
    "            C_distance[n] = np.sqrt(np.sum((C0[:, np.arange(N) != n] - C1[:, np.arange(N) != n])**2))\n",
    "            R_distance[n] = np.sqrt(np.sum((R0 - R1)**2))\n",
    "\n",
    "        results.append({'C_distance': C_distance, 'R_distance': R_distance})\n",
    "    return results\n",
    "\n",
    "for s in [\"s10\",\"s15\",\"s20\"]:\n",
    "    for t in [\"t5\", \"t10\", \"t15\", \"t20\", \"t25\"]:\n",
    "        for iter_num in range(1, 11): \n",
    "            if t == \"t5\":\n",
    "                batch_t = t5.to(device)\n",
    "            elif t == \"t10\":\n",
    "                batch_t = t10.to(device)\n",
    "            elif t == \"t15\":\n",
    "                batch_t = t15.to(device)\n",
    "            elif t == \"t20\":\n",
    "                batch_t = t20.to(device)\n",
    "            elif t == \"t25\":\n",
    "                batch_t = t25.to(device)\n",
    "\n",
    "            input_file_path = f'~/eNODEconstr/simulation/generation_data/n{N}m{M}/{s}/{t}/iter{iter_num}'\n",
    "            output_dir_path = f'~/eNODEconstr/simulation/eNODEconstr/n{N}m{M}/training/{s}/{t}/iter{iter_num}'\n",
    "            pre_dir_path = f'~/eNODEconstr/simulation/eNODEconstr/n{N}m{M}/prediction/{s}/{t}/iter{iter_num}'\n",
    "            \n",
    "            os.makedirs(output_dir_path, exist_ok=True)\n",
    "            os.makedirs(pre_dir_path, exist_ok=True)\n",
    "            \n",
    "            data_file = os.path.join(input_file_path, 'true_initial_state.h5')\n",
    "            with h5py.File(data_file, 'r') as hdf:\n",
    "                data = hdf['true_initial_state'][:]\n",
    "                tensor_data = torch.tensor(data, dtype=torch.float32).permute(0, 2, 1)\n",
    "            \n",
    "            mu_file = os.path.join(input_file_path, 'mu_binary_matrix.csv')\n",
    "            mu_base = pd.read_csv(mu_file, header=None, index_col=False)\n",
    "            mu_base = torch.tensor(mu_base.values, dtype=torch.float32, device=device)\n",
    "\n",
    "            dataset = CustomDataset(tensor_data)\n",
    "            batch_s = int(int(s[1:])* 0.2)\n",
    "            dataloader = DataLoader(dataset, batch_size= batch_s, shuffle=False)\n",
    "            model = ODEFunc(N, M, mu_base).to(device)\n",
    "            model.load_state_dict(torch.load(os.path.join(output_dir_path, 'best_model.pth')))\n",
    "            model.eval()\n",
    "            original_data = tensor_data \n",
    "            def set_column_to_zero_and_predict(data, column_index):\n",
    "                new_data = data.clone()\n",
    "                new_data[:, :, column_index] = 0\n",
    "                predictions = []\n",
    "                with torch.no_grad():\n",
    "                    for i in range(len(new_data)):\n",
    "                        try:\n",
    "                            input_data = new_data[i, 0, :].float().unsqueeze(0).to(device)\n",
    "                            pred_y = odeint(model, input_data, batch_t)  \n",
    "                            predictions.append(pred_y.cpu().numpy())\n",
    "                        except AssertionError as e:\n",
    "                            print(f\"Error with subject index {i}, removing microbe {column_index}: {e}\")\n",
    "                            placeholder = np.full_like(predictions[0], np.nan) if predictions else np.full((len(batch_t), 1, len(new_data[i, 0, :])), np.nan)\n",
    "                            predictions.append(placeholder)\n",
    "                            continue\n",
    "                return predictions\n",
    "\n",
    "            predict = []\n",
    "            for i in tqdm(range(N)):\n",
    "                predictions = set_column_to_zero_and_predict(original_data, i)\n",
    "                predictions_list = predictions \n",
    "                predictions_array = np.array(predictions_list)\n",
    "                predict.append(predictions_array)\n",
    "\n",
    "            predict = np.array(predict)\n",
    "            predict = predict.squeeze(3)\n",
    "            predict = predict.transpose(1,0,2,3)\n",
    "            predict[predict < 0] = 0  \n",
    "            np.save(f'{pre_dir_path}/predict.npy', predict)  \n",
    "\n",
    "            readRDS = robjects.r['readRDS']\n",
    "            true_initial_state = np.array(readRDS(f'{input_file_path}/true_initial_state.rds'))\n",
    "            true_new_state = np.array(readRDS(f'{input_file_path}/true_new_state.rds'))\n",
    "\n",
    "            predict_new_state = predict\n",
    "\n",
    "            def read_csv_no_header(filename):\n",
    "                return pd.read_csv(f'{input_file_path}/{filename}', header=None, index_col=False)\n",
    "\n",
    "            mu_true = read_csv_no_header('mu_true.csv')\n",
    "            l_true = read_csv_no_header('l_true.csv')\n",
    "            rho_true = read_csv_no_header('rho_true.csv')\n",
    "            m_true = read_csv_no_header('m_true.csv')\n",
    "            omega_true = read_csv_no_header('omega_true.csv')\n",
    "            lambda_true = read_csv_no_header('lambda_true.csv')\n",
    "\n",
    "            def calculate_and_save_rmse(model_param, param_true_df, param_name):\n",
    "                param_pred_np = model_param.detach().numpy()\n",
    "                param_pred_df = pd.DataFrame(param_pred_np)\n",
    "                param_pred_df.to_csv(f'{pre_dir_path}/{param_name}_pre.txt', sep='\\t', index=False, header=False)           \n",
    "                param_pred_np = param_pred_df.to_numpy()\n",
    "                param_true_np = param_true_df.to_numpy()\n",
    "                rmse = np.sqrt(np.mean((param_pred_np - param_true_np) ** 2))\n",
    "                r_rmse = rmse / np.sqrt(np.mean((param_true_np) ** 2))\n",
    "                return rmse, r_rmse\n",
    "\n",
    "            results = {}\n",
    "\n",
    "            results['mu'] = calculate_and_save_rmse(model.mu, mu_true, 'mu')\n",
    "            results['l'] = calculate_and_save_rmse(model.l, l_true, 'l')\n",
    "            results['rho'] = calculate_and_save_rmse(model.rho, rho_true, 'rho')\n",
    "            results['m'] = calculate_and_save_rmse(model.m, m_true, 'm')\n",
    "            results['omega'] = calculate_and_save_rmse(model.omega, omega_true, 'omega')\n",
    "            results['lambda'] = calculate_and_save_rmse(model.lambda_, lambda_true, 'lambda')\n",
    "\n",
    "            results_df = pd.DataFrame(results, index=['rmse', 'r_rmse']).transpose()\n",
    "            results_df.to_csv(f'{pre_dir_path}/pe_rmse.txt', sep='\\t', header=True, index=True)\n",
    "\n",
    "            S = int(s[1:])\n",
    "            r_rmse_results = calculate_r_rmse(true_new_state, predict_new_state, S, N, M)\n",
    "            r_rmse_results.to_csv(f'{pre_dir_path}/traj_r_rmse.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "            L2_Distance_results = calculate_L2_Distance(true_initial_state, predict_new_state, S, N, M)\n",
    "            C_l2_d_avg = np.nanmean([result['C_distance'] for result in L2_Distance_results], axis=0)\n",
    "            R_l2_d_avg = np.nanmean([result['R_distance'] for result in L2_Distance_results], axis=0)\n",
    "            microbes = [f\"Microbe{i+1}\" for i in range(N)]\n",
    "            C_l2_df = pd.DataFrame({'Regulate': microbes, 'Score': C_l2_d_avg, 'Group': 'Microbe'})\n",
    "            C_l2_df['Score_normalized'] = C_l2_df['Score'] / (C_l2_df['Score'].sum())\n",
    "            R_l2_df = pd.DataFrame({'Regulate': microbes, 'Score': R_l2_d_avg, 'Group': 'Metabolite'})\n",
    "            R_l2_df['Score_normalized'] = R_l2_df['Score'] / (R_l2_df['Score'].sum())\n",
    "            combined_df = pd.concat([C_l2_df, R_l2_df])\n",
    "            combined_df.to_csv(f'{pre_dir_path}/score_pre_mean_l2.txt', sep='\\t', index=False)\n",
    "\n",
    "            results = calculate_L2_Distance(true_initial_state, predict_new_state, S, N, M)\n",
    "            C_l2_d_list = pd.DataFrame([result['C_distance'] for result in results])\n",
    "            microbes = ['Microbe' + str(i+1) for i in range(N)] \n",
    "            subjects = [f\"Subject{i+1}\" for i in range(S)]  \n",
    "            row_sums = C_l2_d_list.sum(axis=1)\n",
    "            C_l2_normalized = C_l2_d_list.div(row_sums, axis=0)\n",
    "            C_l2_normalized.columns = microbes\n",
    "            C_l2_normalized['Subject'] = subjects\n",
    "            C_l2_normalized_long = pd.melt(C_l2_normalized, id_vars='Subject', var_name='Regulate', value_name='Score')\n",
    "            \n",
    "            C_l2 = C_l2_d_list\n",
    "            C_l2.columns = microbes\n",
    "            C_l2['Subject'] = subjects\n",
    "            C_l2_long = pd.melt(C_l2, id_vars='Subject', var_name='Regulate', value_name='Score')\n",
    "\n",
    "            R_l2_d_list = pd.DataFrame([result['R_distance'] for result in results])    \n",
    "            row_sums = R_l2_d_list.sum(axis=1)\n",
    "            R_l2_normalized = R_l2_d_list.div(row_sums, axis=0)\n",
    "            R_l2_normalized.columns = microbes\n",
    "            R_l2_normalized['Subject'] = subjects\n",
    "            R_l2_normalized_long = pd.melt(R_l2_normalized, id_vars='Subject', var_name='Regulate', value_name='Score')\n",
    "            \n",
    "            R_l2 = R_l2_d_list\n",
    "            R_l2.columns = microbes\n",
    "            R_l2['Subject'] = subjects\n",
    "            R_l2_long = pd.melt(R_l2, id_vars='Subject', var_name='Regulate', value_name='Score') \n",
    "\n",
    "            C_l2_long['Group'] = 'Microbe'\n",
    "            R_l2_long['Group'] = 'Metabolite'\n",
    "            combined_score_l2 = pd.concat([C_l2_long, R_l2_long])\n",
    "            combined_score_l2.to_csv(f'{pre_dir_path}/score_pre_subject_l2.txt', index=False, sep='\\t', header=True)\n",
    "\n",
    "            C_l2_normalized_long['Group'] = 'Microbe'\n",
    "            R_l2_normalized_long['Group'] = 'Metabolite'\n",
    "            combined_score_normalized_l2 = pd.concat([C_l2_normalized_long, R_l2_normalized_long])\n",
    "            combined_score_normalized_l2.to_csv(f'{pre_dir_path}/score_normalized_pre_subject_l2.txt', index=False, sep='\\t', header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "0c434b3bfc52b0dd704385aed352ed7c19408970098de37686e5c0de2e6af02a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
